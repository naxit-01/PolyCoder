{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PolyCoder**\n",
    "zdroj: https://github.com/VHellendoorn/Code-LMs\n",
    "\n",
    "Tři různé úrovně modulů:  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    -PolyCoder-160M  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    -PolyCoder-0.4B  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;    -PolyCoder-2.7B  \n",
    "\n",
    "V této práci je využit model PolyCoder-2.7B  \n",
    "Modely byly trénovány na velkém datasetu kódu zahrnujícího 12 programovacích jazyků. To zahrnuje 2,7B parametrický model (přezdívaný PolyCoder , trénovaný pro 100K a 150K kroků), 405M parametrický model (100K & 150K kroků) a 160M parametrický model (150K kroků)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Datové sady**\n",
    "249GB vícejazyčný dataset. Seznam cest k souborům lze stáhnout z: https://zenodo.org/record/6363556/files/index.zip . Každý řádek v souboru je cesta k souboru spolu s jeho SHA-256 hash, aby se usnadnila deduplikace. To znamená, že hash umožňuje kontrolu, zda soubory z nějaké budoucí testovací sady již byly obsaženy v trénovací sadě.\n",
    "\n",
    "Proces sběru a filtrování dat je podrobně popsán v článku a níže. Konečné, filtrované statistiky datové sady jsou: \n",
    "   \n",
    "![dataset](./dataset.png)  \n",
    "  \n",
    "**Sběr a filtrování dat**\n",
    "\n",
    "V říjnu 2021 byly z GitHubu naklonovány nejoblíbenější úložiště pro 12 oblíbených programovacích jazyků s alespoň 50 hvězdičkami. U každého projektu byl extrahován každý soubor patřící do většinového jazyka tohoto projektu, čímž se získal tréninková sada níže (po vyčištění). Tato počáteční, nefiltrovaná datová sada zahrnovala 631 GB a 38,9 milionů souborů.\n",
    "\n",
    "Dále, podobně jako u Codex a CodeParrot, byly odfiltrovány velmi velké (>1 MB) a velmi krátké (<100 tokenů) soubory, čímž se datová sada zmenšila na 424 GB. Soubory byly poté deduplikovány na základě hashe jejich obsahu, což snížilo počet souborů o dalších přibližně 30 %, takže zůstalo 249 GB dat a 24,1 milionu souborů. Nebyly použity žádné tokenizační filtry; model zpracovává celé soubory včetně všech komentářů."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Hodnoceni**\n",
    "![image.png](./HumanEval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementace řešení**\n",
    "Ověření dostupnosti grafické karty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU: NVIDIA GeForce GTX 1650 with Max-Q Design\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "\n",
    "# Kontrola, zda je dostupné CUDA (což znamená, že máte GPU)\n",
    "if torch.cuda.is_available():  \n",
    "  device = torch.device(\"cuda\")\n",
    "  print('Running on GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "  print('Running on CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Využití knihovny Transformers, která obsahuje tisíce různých předtrénovaných modelů z Hugging Face Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from packaging import version\n",
    "assert version.parse(transformers.__version__) >= version.parse(\"4.23.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z Hugging Face Hub stahuji tokenizer pro PolyCoder2.7B. Tokenizer je nástroj který umí rozdělit vstupní text na jednotlivé, pro model logické, tokeny  \n",
    "  \n",
    "Model PolyCoder2.7B je předtrenované strojevé učení pro generování programovacího kódu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.26s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"NinedayWang/PolyCoder-2.7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"NinedayWang/PolyCoder-2.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Přesunutí modelu na GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def return1():\\n  \"\"\"Returns 1.\"\"\"\\n\n",
    "def convert_to_number(string):\n",
    "def binarySearch(arr, left, right, x):\n",
    "    mid = (left +"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:lightblue\">**input_ids:**</span> Představuje vstupní data pro model. Jsou to tokeny, které byly zakódovány pomocí tokenizeru.  \n",
    "  \n",
    "<span style=\"color:lightblue\">**max_length:**</span> Maximální délka generovaného textu. Pokud je dosaženo této délky, generování se zastaví.  \n",
    "  \n",
    "<span style=\"color:lightblue\">**num_beams:**</span> Počet paprsků použité při paprskovém vyhledávání. Paprskové vyhledávání je technika používaná v autoregresivním generování textu pro nalezení nejpravděpodobnějších výstupních sekvencí.  \n",
    "  \n",
    "<span style=\"color:lightblue\">**num_return_sequences:**</span> Počet sekvencí, které mají být vráceny. Každá sekvence je generována nezávisle na ostatních.  \n",
    "  \n",
    "<span style=\"color:lightblue\">**attention_mask:**</span> Maska, která určuje, na které tokeny by měl model věnovat pozornost1. Hodnota 1 znamená, že model by měl věnovat pozornost danému tokenu, zatímco hodnota 0 znamená, že by měl daný token ignorovat.  \n",
    "  \n",
    "<span style=\"color:lightblue\">**pad_token_id:**</span> Určuje ID tokenu, který se používá pro doplnění (padding) sekvencí. Pokud je sekvence kratší než max_length, pak se použije tento token pro doplnění sekvence na max_length.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public int getTotalWeight(List<Integer> weights) {\n",
      "\t\t// Sum weights in parallel.\n",
      "\t\treturn                                                                                                                            \n",
      "public int getTotalWeight(List<Integer> weights) {\n",
      "\t\t// Sum weights in parallel.\n",
      "\t\treturn                                                                                                                            \n",
      "\n",
      "public int getTotalWeight(List<Integer> weights) {\n",
      "\t\t// Sum weights in parallel.\n",
      "\t\treturn                                                                                                                            //\n",
      "public int getTotalWeight(List<Integer> weights) {\n",
      "\t\t// Sum weights in parallel.\n",
      "\t\treturn                                                                                                                           \n",
      "\t\t\n"
     ]
    }
   ],
   "source": [
    "prompt = '''public int getTotalWeight(List<Integer> weights) {\\n\\t\\t// Sum weights in parallel.\\n\\t\\treturn '''\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Vytvoření attention mask\n",
    "attention_mask = torch.ones(input_ids.shape, device=device)\n",
    "\n",
    "# Přesunutí vstupních dat na GPU\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "result = model.generate(input_ids, max_length=150, num_beams=15, num_return_sequences=4, attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id)\n",
    "for res in result:\n",
    "    print(tokenizer.decode(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def symmetric_encryption(key,plaintext):\n",
      " \"\"\"Returns ciphertext\"\"\"\n",
      " if len(plaintext)!= len(key):\n",
      "    raise ValueError(\"plaintext and key must be of the same length\")\n",
      " if len(plaintext) == 0\n",
      "def symmetric_encryption(key,plaintext):\n",
      " \"\"\"Returns ciphertext\"\"\"\n",
      " if len(plaintext)!= len(key):\n",
      "    raise ValueError(\"plaintext and key must be of the same length\")\n",
      " if len(key)!= 16\n",
      "def symmetric_encryption(key,plaintext):\n",
      " \"\"\"Returns ciphertext\"\"\"\n",
      " if len(plaintext)!= len(key):\n",
      "    raise ValueError(\"plaintext and key must be of the same length\")\n",
      " if len(plaintext)!= len\n",
      "def symmetric_encryption(key,plaintext):\n",
      " \"\"\"Returns ciphertext\"\"\"\n",
      " if len(plaintext)!= len(key):\n",
      "    raise ValueError(\"plaintext and key must be of the same length\")\n",
      " if len(key)!= 8\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "prompt = '''def symmetric_encryption(key,plaintext):\\n \"\"\"Returns ciphertext\"\"\"'''\n",
    "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "# Přesunutí vstupních dat na GPU\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "result = model.generate(input_ids, max_length=50, num_beams=4, num_return_sequences=4)\n",
    "for res in result:\n",
    "    print(tokenizer.decode(res))\n",
    "\n",
    "print(\"end\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
